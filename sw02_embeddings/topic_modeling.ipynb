{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "TF-IDF:\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "\n",
    "print('BOW:')\n",
    "print(corpus_bow[0])\n",
    "\n",
    "print('TF-IDF:')\n",
    "print(corpus_tfidf[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"palestinian\" + 0.004*\"mr\" + 0.003*\"australia\" + 0.003*\"government\" + 0.003*\"australian\" + 0.003*\"south\" + 0.003*\"new\" + 0.003*\"israeli\" + 0.003*\"arafat\" + 0.003*\"afghanistan\"'),\n",
       " (6,\n",
       "  '0.002*\"ses\" + 0.002*\"japan\" + 0.002*\"argentina\" + 0.002*\"hewitt\" + 0.002*\"car\" + 0.002*\"club\" + 0.002*\"roads\" + 0.002*\"road\" + 0.002*\"japanese\" + 0.001*\"crisis\"'),\n",
       " (2,\n",
       "  '0.003*\"hollingworth\" + 0.003*\"dr\" + 0.003*\"governor-general\" + 0.003*\"space\" + 0.002*\"abuse\" + 0.002*\"adventure\" + 0.002*\"school\" + 0.002*\"guides\" + 0.002*\"anglican\" + 0.002*\"canyoning\"'),\n",
       " (9,\n",
       "  '0.007*\"qantas\" + 0.005*\"workers\" + 0.004*\"industrial\" + 0.004*\"maintenance\" + 0.003*\"unions\" + 0.003*\"dispute\" + 0.002*\"freeze\" + 0.002*\"relations\" + 0.002*\"wage\" + 0.002*\"airline\"'),\n",
       " (7,\n",
       "  '0.003*\"firefighters\" + 0.002*\"zimbabwe\" + 0.002*\"fires\" + 0.002*\"service\" + 0.002*\"pay\" + 0.002*\"rural\" + 0.002*\"rates\" + 0.002*\"storm\" + 0.002*\"homes\" + 0.002*\"lording\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "model_lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10, passes=20, iterations=400)\n",
    "\n",
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics:\n",
      "(0, '0.004*\"palestinian\" + 0.004*\"mr\" + 0.003*\"australia\" + 0.003*\"government\" + 0.003*\"australian\"')\n",
      "(1, '0.002*\"warne\" + 0.002*\"innings\" + 0.002*\"wicket\" + 0.002*\"asic\" + 0.002*\"kallis\"')\n",
      "(2, '0.003*\"hollingworth\" + 0.003*\"dr\" + 0.003*\"governor-general\" + 0.003*\"space\" + 0.002*\"abuse\"')\n",
      "(3, '0.003*\"metres\" + 0.002*\"karzai\" + 0.002*\"kandahar\" + 0.002*\"event\" + 0.002*\"petrol\"')\n",
      "(4, '0.002*\"friedli\" + 0.002*\"replied\" + 0.002*\"hih\" + 0.002*\"projects\" + 0.001*\"related\"')\n",
      "(5, '0.003*\"reid\" + 0.002*\"cancer\" + 0.002*\"child\" + 0.002*\"sergeant\" + 0.002*\"lung\"')\n",
      "(6, '0.002*\"ses\" + 0.002*\"japan\" + 0.002*\"argentina\" + 0.002*\"hewitt\" + 0.002*\"car\"')\n",
      "(7, '0.003*\"firefighters\" + 0.002*\"zimbabwe\" + 0.002*\"fires\" + 0.002*\"service\" + 0.002*\"pay\"')\n",
      "(8, '0.002*\"labor\" + 0.002*\"gang\" + 0.002*\"factory\" + 0.002*\"goshen\" + 0.001*\"pacific\"')\n",
      "(9, '0.007*\"qantas\" + 0.005*\"workers\" + 0.004*\"industrial\" + 0.004*\"maintenance\" + 0.003*\"unions\"')\n"
     ]
    }
   ],
   "source": [
    "topics = model_lda.print_topics(num_words=5)\n",
    "print('LDA topics:')\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(model_lda[corpus_tfidf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_representation(query):\n",
    "    query = preprocess(query)\n",
    "    query_bow = dictionary.doc2bow(query)\n",
    "    query_tfidf = model_tfidf[query_bow]\n",
    "    query_lda = model_lda[query_tfidf]\n",
    "    return query_lda\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 276 Similarity: 0.99407464\n",
      "Defence Minister Robert Hill has confirmed Australian troops arrived in Afghanistan this morning. Senator Hill says it is an advance party and the rest of the troops will arrive within the next few days. He says Australian forces will operate with US troops in southern Afghanistan to fight the Taliban and Al Qaeda networks. Senator Hill says the operation could take several months. \n",
      "\n",
      "Document: 280 Similarity: 0.99380845\n",
      "The Greens have officially won their second Senate spot in Federal Parliament. The Senate count for New South Wales has been finalised with Kerry Nettle from the Greens taking the final position from long time Democrats Senator Vicki Bourne. Senator Bourne says she is very lucky to have served in the Parliament for 12 years and has nominated serving as an observer at the East Timor independence ballot as the high point of her career. She has wished Kerry Nettle well, saying it is a great honour and a great responsibility to be elected to the Senate. \n",
      "\n",
      "Document: 162 Similarity: 0.9935192\n",
      "The Federal Agriculture Minister, Warren Truss, says he has not been able to win any changes to the farm bill being debated by the United States Congress. Mr Truss has led a delegation of Australian farmers to Washington lobbying for some of the Government subsidies to farmers to be removed. He says he has not achieved any changes to the amount of Government protection for US farmers. Mr Truss says that will mean Australian farmers will suffer. \"We're especially concerned at the clear intent of the farm lobby to seek to entrench a mentality of farm subsidies in the USA. \"It is obvious that the US, which was once proudly boasted to be the most efficient farmers in the world, have now degenerated to a situation where US farmers are dependent on the taxpayers for around half their income,\" Mr Truss said. \n",
      "\n",
      "Document: 205 Similarity: 0.993206\n",
      "Australian's casinos generated a $3.1 billion income in the 2000-2001 financial year. The Australian Bureau of Statistics has found gambling was the biggest money winner for casinos, making up 80 per cent or $2.5 billion of total income. Governments also did well, taking more than $500 million from the casinos for gambling taxes and levies while the 20,000 employees were paid more than $800 million for their work. But despite the expense, the profit for Australian casinos increased by 19 per cent for the year. At the end of June this year, there was almost 11,000 poker and gaming machines and more than 1,000 gaming tables across Australia. \n",
      "\n",
      "Document: 72 Similarity: 0.9930872\n",
      "Australian cricket selectors have made just one change to the squad that beat South Africa in the opening Test for the second Test beginning in Melbourne on Boxing Day. As predicted, Queensland pace bowler Andy Bichel replaces spin bowler Stuart MacGill, who was 12th man for the Adelaide Test. MacGill took five wickets for New South Wales on day one of the tour match against South Africa at the SCG yesterday, but it was not enough to sway selectors. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Prime Minister of Australia'\n",
    "query_lda = lda_representation(query)\n",
    "sims = index[query_lda]\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i, sim in sims[:5]:\n",
    "    print('Document:', i, 'Similarity:', sim)\n",
    "    print(articles_orig[i])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
