{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "TF-IDF:\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "\n",
    "print('BOW:')\n",
    "print(corpus_bow[0])\n",
    "\n",
    "print('TF-IDF:')\n",
    "print(corpus_tfidf[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  '0.002*\"mr\" + 0.002*\"zealand\" + 0.001*\"arafat\" + 0.001*\"palestinian\" + 0.001*\"rafter\" + 0.001*\"people\" + 0.001*\"asked\" + 0.001*\"asylum\" + 0.001*\"israeli\" + 0.001*\"secret\"'),\n",
       " (1,\n",
       "  '0.003*\"workers\" + 0.002*\"qantas\" + 0.002*\"best\" + 0.002*\"industrial\" + 0.002*\"us\" + 0.001*\"unions\" + 0.001*\"yallourn\" + 0.001*\"hewitt\" + 0.001*\"power\" + 0.001*\"trees\"'),\n",
       " (2,\n",
       "  '0.003*\"palestinian\" + 0.002*\"israel\" + 0.002*\"hamas\" + 0.002*\"party\" + 0.001*\"arafat\" + 0.001*\"suicide\" + 0.001*\"minister\" + 0.001*\"israeli\" + 0.001*\"two\" + 0.001*\"city\"'),\n",
       " (8,\n",
       "  '0.002*\"club\" + 0.002*\"commission\" + 0.001*\"afghanistan\" + 0.001*\"lockett\" + 0.001*\"us\" + 0.001*\"bin\" + 0.001*\"laden\" + 0.001*\"mr\" + 0.001*\"south\" + 0.001*\"year\"'),\n",
       " (7,\n",
       "  '0.002*\"governor-general\" + 0.002*\"hollingworth\" + 0.002*\"test\" + 0.002*\"dr\" + 0.001*\"woomera\" + 0.001*\"centre\" + 0.001*\"republic\" + 0.001*\"child\" + 0.001*\"people\" + 0.001*\"oil\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "\n",
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics:\n",
      "(0, '0.003*\"palestinian\" + 0.002*\"government\" + 0.002*\"afghanistan\" + 0.002*\"states\" + 0.001*\"-\"')\n",
      "(1, '0.002*\"workers\" + 0.002*\"company\" + 0.002*\"pakistan\" + 0.002*\"friedli\" + 0.002*\"indian\"')\n",
      "(2, '0.002*\"arafat\" + 0.002*\"palestinian\" + 0.001*\"israeli\" + 0.001*\"change\" + 0.001*\"staff\"')\n",
      "(3, '0.002*\"reid\" + 0.002*\"man\" + 0.002*\"police\" + 0.001*\"hotel\" + 0.001*\"fire\"')\n",
      "(4, '0.003*\"palestinian\" + 0.002*\"israeli\" + 0.002*\"arafat\" + 0.001*\"australia\" + 0.001*\"government\"')\n",
      "(5, '0.002*\"test\" + 0.002*\"club\" + 0.002*\"afghanistan\" + 0.002*\"us\" + 0.002*\"south\"')\n",
      "(6, '0.002*\"palestinian\" + 0.002*\"dr\" + 0.002*\"afghan\" + 0.001*\"australia\" + 0.001*\"agreement\"')\n",
      "(7, '0.001*\"bill\" + 0.001*\"workers\" + 0.001*\"royal\" + 0.001*\"australia\" + 0.001*\"qantas\"')\n",
      "(8, '0.002*\"yacht\" + 0.002*\"australia\" + 0.002*\"asic\" + 0.002*\"hobart\" + 0.002*\"sydney\"')\n",
      "(9, '0.002*\"qantas\" + 0.002*\"workers\" + 0.002*\"centre\" + 0.002*\"woomera\" + 0.002*\"detainees\"')\n"
     ]
    }
   ],
   "source": [
    "topics = model_lda.print_topics(num_words=5)\n",
    "print('LDA topics:')\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(model_lda[corpus_tfidf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_representation(query):\n",
    "    query = preprocess(query)\n",
    "    query_bow = dictionary.doc2bow(query)\n",
    "    query_tfidf = model_tfidf[query_bow]\n",
    "    query_lda = model_lda[query_tfidf]\n",
    "    return query_lda\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 207 Similarity: 0.99473554\n",
      "Geoff Huegill has continued his record-breaking ways at the World Cup short course swimming in Melbourne, bettering the Australian record in the 100 metres butterfly. Huegill beat fellow Australian Michael Klim, backing up after last night setting a world record in the 50 metres butterfly. \n",
      "\n",
      "Document: 276 Similarity: 0.99407655\n",
      "Defence Minister Robert Hill has confirmed Australian troops arrived in Afghanistan this morning. Senator Hill says it is an advance party and the rest of the troops will arrive within the next few days. He says Australian forces will operate with US troops in southern Afghanistan to fight the Taliban and Al Qaeda networks. Senator Hill says the operation could take several months. \n",
      "\n",
      "Document: 297 Similarity: 0.99315614\n",
      "The Federal National Party has rejected a possible merger with the Liberals' at this stage, but it has not ruled out the option over the next three years. Liberal Party President Shane Stone is reported as saying amalgamation has to be considered as a strategy for the future of the Coalition. It comes as the two parties fight over numbers and muscle within the parliamentary groupings of the Coalition. National Party President Helen Dickie says merging the two parties is not necessary. \"I guess you cannot categorically rule out anything. There will be discussions by all states, but the states at this stage have led me to understand that certainly amalgamation for them is not an issue,\" she said. \n",
      "\n",
      "Document: 95 Similarity: 0.9929609\n",
      "Legal abortion in Tasmania is one step closer with the lower house of Parliament voting to change the state's abortion laws. After a marathon 16-hour debate,  the House of Assembly this morning passed by 15 votes to eight, a Private Member's Bill which would allow medically sanctioned abortions. Debate on the bill began at 11:30am AEDT yesterday and at 5:00am today Speaker Michael Polley declared a result. Just before 10:00pm, the Deputy Premier Paul Lennon, Police Minister David Llewellyn and Labor backbencher Steven Kons abstained from voting on the bill's second reading. Then came seven hours of debate on proposed amendments calling for further expert opinion, post procedure counselling, cooling off periods and calls to stop the bill being retrospective. But apart from an amendment to ensure a specialist assessment, and written consent for the procedure, the bill passed unchanged. The Legislative Council will begin debate on the bill at 9:00am today. \n",
      "\n",
      "Document: 136 Similarity: 0.9929368\n",
      "A new report suggests the costs of an aging Australian population have been exaggerated. The report issued by the Australia Institute says a detailed examination of population and health data shows an aging population will not create an unsustainable burden on a shrinking workforce. Far from being an economic and social burden, it found the majority of older people enjoyed healthy and independent lives, many making financial contributions to their families and participating in voluntary community activities. The paper challenges the assumption an older population will see health costs rise to unsustainable levels. It says rising health costs are caused mainly by factors other than aging such as the growth of medical technology, rising consumer demand and escalating prices. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Prime Minister of Australia'\n",
    "query_lda = lda_representation(query)\n",
    "sims = index[query_lda]\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i, sim in sims[:5]:\n",
    "    print('Document:', i, 'Similarity:', sim)\n",
    "    print(articles_orig[i])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
