{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df666b",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN with Attention\n",
    "We will now add attention to our sequence-to-sequence RNN. There are several ways to incorporate the context vector $c$ into the RNN architecture:\n",
    "1. Add an additional term to the computation of the gates/states (i.e. treat it as an input just like $h^{(dec)}_{t-1}$ and $y_t$). This was used in the original paper (Bahdanau et al, 2015), described in Appendix A.\n",
    "2. Concatenate it with the hidden state of the last time step $h^{(dec)}_{t-1}$ and project the concatenation down from `encoder_output_dim + decoder_hidden_dim` to `decoder_hidden_dim`.\n",
    "3. Concatenate it with the input $y_t$.\n",
    "\n",
    "We will use variant 2 in this exercise. We'll make our lives a bit easier by implementing a 1-layer decoder and working with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eae6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420803d",
   "metadata": {},
   "source": [
    "Since we have to compute the context vector at every step, we can't use the high-level `nn.LSTM` interface by PyTorch. We first implement a decoder LSTM class that operates an `nn.LSTMCell`. We start with the `__init__` method where we initialize all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = dec_hidden_dim\n",
    "        self.project = nn.Linear(enc_output_dim + dec_hidden_dim, dec_hidden_dim, bias=False)\n",
    "        self.v = nn.Parameter(torch.empty(dec_hidden_dim))\n",
    "        self.W = nn.Parameter(torch.zeros(dec_hidden_dim, dec_hidden_dim))\n",
    "        self.U = nn.Linear(enc_output_dim, dec_hidden_dim, bias=False)  # can use parameter or Linear layer\n",
    "        self.cell = nn.LSTMCell(input_dim, dec_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9186",
   "metadata": {},
   "source": [
    "Add a `reset_parameters` method that initializes all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9deec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self):\n",
    "    self.project.reset_parameters()\n",
    "    nn.init.normal_(self.v, mean=0, std=1)\n",
    "    nn.init.normal_(self.W, mean=0, std=1)\n",
    "    self.U.reset_parameters()\n",
    "    self.cell.reset_parameters()\n",
    "\n",
    "DecoderLSTMWithAttention.reset_parameters = reset_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5ce41",
   "metadata": {},
   "source": [
    "Add a `forward` method that takes a sequence `y` and encoder hidden states `encoder_hidden_states` as input. `encoder_hidden_states` is a tensor of size `[sequence_length, encoder_output_dim]`, where `encoder_output_dim = num_directions * encoder_hidden_dim`. The `forward` method should call `compute_context_vector` that computes the attention-weighted context vector. We will implement it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd58a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, y, encoder_hidden_states):\n",
    "    hidden_state = torch.zeros(self.hidden_dim)\n",
    "    cell_state = torch.zeros(self.hidden_dim)\n",
    "    outputs = []\n",
    "    \n",
    "    # loop over the target sequence\n",
    "    for y_i in y:\n",
    "        context_vector = self.compute_context_vector(hidden_state, encoder_hidden_states)\n",
    "        concatenated = torch.cat([hidden_state, context_vector], dim=-1)\n",
    "        hidden_state = self.project(concatenated)  # down-project to dec_hidden_dim\n",
    "        hidden_state, cell_state = self.cell(y_i, (hidden_state, cell_state))\n",
    "        outputs.append(hidden_state)\n",
    "    \n",
    "    return torch.stack(outputs), (hidden_state, cell_state)\n",
    "\n",
    "DecoderLSTMWithAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc3b86",
   "metadata": {},
   "source": [
    "Now it's time to implement the `compute_context_vector` function. Its inputs are `previous_decoder_hidden_state` and `encoder_hidden_states`. Use either additive or multiplicative attention, as we saw it in the course. Extend the trainable parameters in your `__init__` method if necessary and initialize them in `reset_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb43d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):\n",
    "    # Compute a scalar weight for each encoder hidden state\n",
    "    attention_scores = []\n",
    "    for encoder_hidden_state in encoder_hidden_states:\n",
    "        attention_scores.append(self.v @ torch.tanh(self.W @ previous_decoder_hidden_state + self.U(encoder_hidden_state)))\n",
    "    attention_scores = torch.stack(attention_scores)\n",
    "    # alternative: efficient solution with matrix operations\n",
    "    # attention_scores = self.v @ torch.tanh(self.W(previous_decoder_hidden_state) + self.U(encoder_hidden_states)).T\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    context_vector = attention_weights @ encoder_hidden_states\n",
    "    return context_vector\n",
    "\n",
    "DecoderLSTMWithAttention.compute_context_vector = compute_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707da1eb",
   "metadata": {},
   "source": [
    "**Sequence-to-sequence model.** We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88ff358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically, encoder/decoder hidden dimensions are the same,\n",
    "# but here we choose them differently to test our implementation.\n",
    "embedding_dim = 10\n",
    "enc_hidden_dim = 15\n",
    "dec_hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_directions = 2 if bidirectional else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138382fc",
   "metadata": {},
   "source": [
    "Now we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8607e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, enc_hidden_dim, num_enc_layers, bidirectional, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        enc_output_dim = enc_hidden_dim * num_directions\n",
    "\n",
    "        # define the encoder and decoder    \n",
    "        self.encoder = nn.LSTM(embedding_dim, enc_hidden_dim, num_layers=num_layers, bidirectional=bidirectional)\n",
    "        self.decoder = DecoderLSTMWithAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "\n",
    "        # initialize their parameters\n",
    "        self.encoder.reset_parameters()\n",
    "        self.decoder.reset_parameters()\n",
    "\n",
    "    def forward(self, x, y, h0, c0):\n",
    "        encoder_hidden_states, _ = self.encoder(x, (h0, c0))\n",
    "        outputs, (hn, cn) = self.decoder(y, encoder_hidden_states)\n",
    "        return outputs, (hn, cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3038",
   "metadata": {},
   "source": [
    "Try your Module with an example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2seqLSTMWithAttention(embedding_dim, enc_hidden_dim, num_layers, bidirectional, dec_hidden_dim)\n",
    "x = torch.randn(10, embedding_dim)\n",
    "y = torch.randn(8, embedding_dim)\n",
    "h0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "c0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d471622",
   "metadata": {},
   "source": [
    "Create a subclass of your decoder LSTM that implements the other type of attention (additive or multiplicative) that you haven't implemented above. What do you need to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e999d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you implemented multiplicative attention first, add parameters v and U in the __init__ method as well.\n",
    "\n",
    "class DecoderLSTMWithMultiplicativeAttention(DecoderLSTMWithAttention):\n",
    "    \n",
    "    def __init__(self, input_dim, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__(input_dim, enc_hidden_dim, dec_hidden_dim)\n",
    "        self.W = nn.Parameter(torch.empty(dec_hidden_dim, enc_hidden_dim))  # overwrite W\n",
    "    \n",
    "    def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):\n",
    "        enc_hidden_state_weights = torch.matmul(\n",
    "            previous_decoder_hidden_state,\n",
    "            torch.matmul(self.W, encoder_hidden_states.T)\n",
    "            # alternatively, we can just use U as defined above\n",
    "            # self.U(encoder_hidden_states).T\n",
    "        )\n",
    "        attention_weights = F.softmax(enc_hidden_state_weights, dim=-1)  # convert to attention weights/probabilities\n",
    "        context_vector = torch.matmul(attention_weights, encoder_hidden_states)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f56e8",
   "metadata": {},
   "source": [
    "We can test our implementation with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bc9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output_dim = enc_hidden_dim * num_directions\n",
    "model.decoder = DecoderLSTMWithMultiplicativeAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "model.decoder.reset_parameters()\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
