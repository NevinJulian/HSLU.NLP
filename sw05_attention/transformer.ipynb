{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2fab24",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "We will implement the Transformer architecture presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86ec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b41c4",
   "metadata": {},
   "source": [
    "We start with the attention. Define a class `TransformerAttention` that will contain all the functions related to the Transformer's attention that we need. Add an `__init__` method that takes `hidden_dim` and `num_heads` as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        #self.head_dim = hidden_dim // num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07441c1e",
   "metadata": {},
   "source": [
    "Now we're adding its functions one after the other. We start with the best part: the attention function. Implement scaled-dot product attention when given `query`, `key`, and `value` tensors as inputs. The dimensions of these tensors are: `[batch_size, sequence_length, head_dim]`. Scaled dot-product attention is defined as:\n",
    "$$\\text{DPA}(Q, K, V) = \\text{softmax}(\\frac{Q K^\\top}{\\sqrt{d}}) V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d266fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dot_product_attention(self, query, key, value):\n",
    "    # [seq_len, head_dim] x [head_dim, seq_len] -> [seq_len, seq_len]\n",
    "    # query: [batch_size, seq_len, head_dim]\n",
    "    # key: [batch_size, seq_len, head_dim]\n",
    "    # value: [batch_size, seq_len, head_dim]\n",
    "    # query x key.T --> dircetly does not work\n",
    "    head_dim = self.hidden_dim // self.num_heads\n",
    "    attention_scores = query @ key.transpose(-1, -2) # [batch_size, seq_len, seq_len]\n",
    "    attention_scores = attention_scores / math.sqrt(head_dim)\n",
    "\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1) # [batch_size, seq_len, seq_len]\n",
    "    output = attention_weights @ value # [batch_size, seq_len, head_dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "TransformerAttention.dot_product_attention = dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0397dd",
   "metadata": {},
   "source": [
    "Implement a function `split_to_heads` that takes a tensor of dimensions `[?, ?, hidden_dim]` and splits it into `num_heads` tensors of size `[?, ?, head_dim]`, where $\\text{head\\_dim} = \\frac{\\text{hidden\\_dim}}{\\text{num\\_heads}}$. The `?` dimensions are the same as before, but your implementation should be independent of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acfa006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_heads(self, tensor):\n",
    "    assert self.hidden_dim % self.num_heads == 0, \"Hidden dim must be divisible by the number of heads\"\n",
    "    head_dim = self.hidden_dim // self.num_heads\n",
    "    return tensor.split(head_dim, dim=-1) # [batch_size, seq_len, hidden_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
    "\n",
    "TransformerAttention.split_to_heads = split_to_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba143",
   "metadata": {},
   "source": [
    "Now implement the `forward` method of `TransformerAttention` (and extend the `__init__` method if necessary). It should:\n",
    "1. project its inputs into `query`, `key` and `value` tensors with 3 separate linear layers\n",
    "2. split the tensors into chunks for each head to process\n",
    "3. perform attention for each head separately\n",
    "4. concatenate the results\n",
    "5. run the output through another linear layer\n",
    "\n",
    "Step 1 and 2 look reversed from the diagram we saw in class, but this is more intuitive and also how Hugging Face implements these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5af616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, hidden_dim, num_heads):\n",
    "    super(TransformerAttention, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "def forward(self, x):\n",
    "    # 1. project x to q,k,v\n",
    "    q = self.query_projection(x)\n",
    "    k = self.key_projection(x)\n",
    "    v = self.value_projection(x) # [batch_size, seq_len, hidden_dim]\n",
    "    \n",
    "    # 2. split to num_head tensors\n",
    "    h_q = self.split_to_heads(q) # num_heads * [batch_size, seq_len, head_dim]\n",
    "    h_k = self.split_to_heads(k)\n",
    "    h_v = self.split_to_heads(v)\n",
    "\n",
    "    # 3. apply dot product attention\n",
    "    attention_outputs = []\n",
    "    for h_q, h_k, h_v in zip(h_q, h_k, h_v):\n",
    "        attention_outputs.append(self.dot_product_attention(h_q, h_k, h_v))\n",
    "    #list of length num_heads with tensors of shape: [batch_size, seq_len, head_dim]\n",
    "    \n",
    "    # 4. concat heads\n",
    "    attention_output_tensor = torch.cat(attention_outputs, dim=-1) # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "    # 5. linear layer\n",
    "    output = self.output_projection(attention_output_tensor) # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "TransformerAttention.__init__ = __init__\n",
    "TransformerAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca8c8c",
   "metadata": {},
   "source": [
    "Create a class `TransformerAttentionBlock` that runs Transformer attention, then adds the input as a residual to the output and performs layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = TransformerAttention(hidden_dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x)\n",
    "        output = self.layer_norm(x + attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1712a",
   "metadata": {},
   "source": [
    "Create a class `FeedForwardNetwork` that consists of two linear layers with a ReLU in between. Also add a residual connection from the input to the output and apply layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5130273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, inner_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, inner_dim)\n",
    "        self.fc2 = nn.Linear(inner_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.layer_norm(x + output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051c66b",
   "metadata": {},
   "source": [
    "Now we can combine the `TransformerAttentionBlock` and the `FeedForwardNetwork` into a `TransformerLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f85aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim, ffn_inner_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.self_attention(x)\n",
    "        output = self.ffn(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678fa37",
   "metadata": {},
   "source": [
    "We are ready to compose our `TransformerEncoder` of a given number of `TransformerLayer`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d99df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerLayer(hidden_dim, ffn_inner_dim, num_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a433083",
   "metadata": {},
   "source": [
    "Let's test our implementation with the hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "642b5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "embedding_dim = hidden_dim\n",
    "ffn_dim = 100\n",
    "num_heads = 4\n",
    "num_encoder_layers = 6\n",
    "batch_size = 2\n",
    "x_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdae29",
   "metadata": {},
   "source": [
    "... and check if it produces the correct output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b69461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, x_len, embedding_dim)\n",
    "encoder = TransformerEncoder(hidden_dim, ffn_dim, num_encoder_layers, num_heads)\n",
    "output = encoder(x)\n",
    "assert list(output.shape) == [batch_size, x_len, hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f341c2",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "For the Transformer decoder, two components are missing.\n",
    "1. A causal mask in the `TransformerAttention`.\n",
    "2. A cross-attention module in the `TransformerLayer`.\n",
    "\n",
    "We start by generalizing the `TransformerAttention` class to use a causal mask in `dot_product_attention` if it is used for decoder self-attention. We check this by accessing an `is_decoder_self_attention` attribute of `self`, which we have to add as an argument to `TransformerAttention`'s `__init__` method first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662da648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an `is_decoder_self_attention` attribute to TransformerAttention.__init__\n",
    "def __init__(self, hidden_dim, num_heads, is_decoder_self_attention=False):\n",
    "    super(TransformerAttention, self).__init__()  # we get an error here if we call super().__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.is_decoder_self_attention = is_decoder_self_attention\n",
    "\n",
    "    self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "TransformerAttention.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ed44eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dot_product attention to use a causal mask in case it is used in the decoder self-attention.\n",
    "def dot_product_attention(self, query, key, value):\n",
    "        head_dim = self.hidden_dim // self.num_heads\n",
    "        attention_scores = query @ key.transpose(-1, -2) # [batch_size, seq_len, seq_len]\n",
    "        attention_scores = attention_scores / math.sqrt(head_dim)\n",
    "        \n",
    "        if self.is_decoder_self_attention:\n",
    "                seq_len = attention_scores.size(1)\n",
    "                mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(attention_scores.device)\n",
    "                attention_scores = attention_scores.masked_fill(mask.to(torch.bool), float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1) # [batch_size, seq_len, seq_len]\n",
    "        output = attention_weights @ value # [batch_size, seq_len, head_dim]\n",
    "\n",
    "        return output\n",
    "\n",
    "TransformerAttention.dot_product_attention = dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba48c06",
   "metadata": {},
   "source": [
    "Now we add cross-attention. We do this by updating the `TransformerAttention`'s `forward` method to take `encoder_hidden_states` as an optional input. Check the lecture slides to see which input gets projected into queries, keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b5a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, encoder_hidden_states=None):\n",
    "    # 1. project x to q,k,v\n",
    "    q = self.query_projection(x)\n",
    "    if encoder_hidden_states is None:\n",
    "        k = self.key_projection(x)\n",
    "        v = self.value_projection(x)\n",
    "    else:\n",
    "        k = self.key_projection(encoder_hidden_states)\n",
    "        v = self.value_projection(encoder_hidden_states)\n",
    "    \n",
    "    # 2. split to num_head tensors\n",
    "    h_q = self.split_to_heads(q) # num_heads * [batch_size, seq_len, head_dim]\n",
    "    h_k = self.split_to_heads(k)\n",
    "    h_v = self.split_to_heads(v)\n",
    "\n",
    "    # 3. apply dot product attention\n",
    "    attention_outputs = []\n",
    "    for h_q, h_k, h_v in zip(h_q, h_k, h_v):\n",
    "        attention_outputs.append(self.dot_product_attention(h_q, h_k, h_v))\n",
    "    #list of length num_heads with tensors of shape: [batch_size, seq_len, head_dim]\n",
    "    \n",
    "    # 4. concat heads\n",
    "    attention_output_tensor = torch.cat(attention_outputs, dim=-1) # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "    # 5. linear layer\n",
    "    output = self.output_projection(attention_output_tensor) # [batch_size, seq_len, hidden_dim]\n",
    "\n",
    "    return output\n",
    "    \n",
    "TransformerAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f0aed",
   "metadata": {},
   "source": [
    "We have to extend the `TransformerAttentionBlock` to allow that additional argument in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "987baf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, encoder_hidden_states=None):\n",
    "    output = self.attention(x, encoder_hidden_states)\n",
    "    output = self.layer_norm(x + output)\n",
    "    return output\n",
    "\n",
    "TransformerAttentionBlock.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c6a5",
   "metadata": {},
   "source": [
    "Now we implement a `TransformerDecoderLayer` that consists of decoder self-attention, cross-attention and a feed-forward network. In the `forward` method, use the encoder hidden states as inputs to the cross-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c3f2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.self_attention.attention.is_decoder_self_attention = True\n",
    "        self.cross_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim, ffn_inner_dim)\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states):\n",
    "        output = self.self_attention(x)\n",
    "        output = self.cross_attention(output, encoder_hidden_states)\n",
    "        output = self.ffn(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133819c",
   "metadata": {},
   "source": [
    "Add a `TransformerDecoder` that holds the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c00668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(hidden_dim, ffn_inner_dim, num_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_hidden_states)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50442dd4",
   "metadata": {},
   "source": [
    "## Transformer Seq2seq Model\n",
    "We can now put everything together. Create and instantiate a Transformer model that encodes a random input `x`, then generates an output hidden representation for each decoder input `y` that we could then feed into a classifier to predict the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e31aa870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_dim, num_encoder_layers, num_decoder_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(hidden_dim, ffn_dim, num_encoder_layers, num_heads)\n",
    "        self.decoder = TransformerDecoder(hidden_dim, ffn_dim, num_decoder_layers, num_heads)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        encoder_hidden_states = self.encoder(x)\n",
    "        output = self.decoder(y, encoder_hidden_states)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9995c",
   "metadata": {},
   "source": [
    "We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5db22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "embedding_dim = hidden_dim\n",
    "ffn_dim = 100\n",
    "num_heads = 4\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 2\n",
    "batch_size = 2\n",
    "x_len = 10\n",
    "y_len = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c5f68",
   "metadata": {},
   "source": [
    "Now we can run our model and test that the output dimensions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fa6d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, x_len, embedding_dim)\n",
    "y = torch.randn(batch_size, y_len, embedding_dim)\n",
    "model = TransformerModel(hidden_dim, ffn_dim, num_encoder_layers, num_decoder_layers, num_heads)\n",
    "output = model(x, y)\n",
    "assert list(output.shape) == [batch_size, y_len, hidden_dim], \"Wrong output shape\"\n",
    "num_model_params = sum(param.numel() for param in model.parameters())\n",
    "assert num_model_params == 50480, f\"Wrong number of parameters: {num_model_params}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071506d",
   "metadata": {},
   "source": [
    "## What is missing for a real implementation?\n",
    "Look at the [implementation of the Transformer layer for BERT by HuggingFace](https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/models/bert/modeling_bert.py#L223), from line 223 until 641.\n",
    "\n",
    "**Question:** Name the things you see HuggingFace's implementation do that is still missing in your own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec33e1b",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "possible corrections: \n",
    "- Dropout\n",
    "- Positional encoding\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
