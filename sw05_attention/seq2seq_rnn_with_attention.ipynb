{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df666b",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN with Attention\n",
    "We will now add attention to our sequence-to-sequence RNN. There are several ways to incorporate the context vector $c$ into the RNN architecture:\n",
    "1. Add an additional term to the computation of the gates/states (i.e. treat it as an input just like $h_{t-1}$ and $x_t$). This was used in the original paper (Bahdanau et al, 2015), described in Appendix A.\n",
    "2. Concatenate it with the hidden state of the last time step $h_{t-1}$ and project the concatenation down from `enc_hidden_dim + dec_hidden_dim` to `dec_hidden_dim`.\n",
    "3. Concatenate it with the input $x_t$ and downproject it.\n",
    "\n",
    "We will use variant 2 in this exercise. We'll make our lives a bit easier by implementing a 1-layer decoder and working with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eae6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420803d",
   "metadata": {},
   "source": [
    "Since we have to compute the context vector at every step, we can't use the high-level `nn.LSTM` interface by PyTorch. We first implement a decoder LSTM class that operates an `nn.LSTMCell`. We start with the `__init__` method where we initialize all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = dec_hidden_dim\n",
    "        self.down_project = nn.Linear(enc_output_dim + dec_hidden_dim, dec_hidden_dim, bias=False)\n",
    "        self.v = nn.Parameter(torch.empty(dec_hidden_dim))\n",
    "        self.W = nn.Parameter(torch.zeros(dec_hidden_dim, dec_hidden_dim))\n",
    "        self.U = nn.Linear(enc_output_dim, dec_hidden_dim, bias=False)  # can use parameter or Linear layer\n",
    "        self.cell = nn.LSTMCell(input_dim, dec_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9186",
   "metadata": {},
   "source": [
    "Add a `reset_parameters` method that initializes all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9deec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(self):\n",
    "    self.down_project.reset_parameters()\n",
    "    nn.init.normal_(self.v, mean=0, std=1)\n",
    "    nn.init.normal_(self.W, mean=0, std=1)\n",
    "    self.U.reset_parameters()\n",
    "    self.cell.reset_parameters()\n",
    "\n",
    "DecoderLSTMWithAttention.reset_parameters = reset_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5ce41",
   "metadata": {},
   "source": [
    "Add a `forward` method that takes a sequence `y` and encoder hidden states `encoder_hidden_states` as input. `encoder_hidden_states` is a tensor of size `[sequence_length, encoder_output_dim]`, where `encoder_output_dim = num_directions * encoder_hidden_dim`. The `forward` method should call `compute_context_vector` that computes the attention-weighted context vector. We will implement it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd58a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, y, encoder_hidden_states):\n",
    "    outputs = []\n",
    "    previous_decoder_hidden_state = torch.zeros(self.hidden_dim)\n",
    "    cell_state = torch.zeros(self.hidden_dim)\n",
    "    \n",
    "    # iterate over sequence y\n",
    "    for y_i in y:\n",
    "        context_vector = self.compute_context_vector(previous_decoder_hidden_state, encoder_hidden_states)\n",
    "        concatenated = torch.cat([previous_decoder_hidden_state, context_vector], dim=-1)\n",
    "        projected = self.down_project(concatenated)\n",
    "        previous_decoder_hidden_state, cell_state = self.cell(y_i, (projected, cell_state))\n",
    "        outputs.append(previous_decoder_hidden_state)\n",
    "    \n",
    "    return torch.stack(outputs), (previous_decoder_hidden_state, cell_state)\n",
    "    \n",
    "DecoderLSTMWithAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc3b86",
   "metadata": {},
   "source": [
    "Now it's time to implement the `compute_context_vector` function. Its inputs are `previous_decoder_hidden_state` and `encoder_hidden_states`. Use either additive or multiplicative attention, as we saw it in the course. Extend the trainable parameters in your `__init__` method if necessary and initialize them in `reset_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb43d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):\n",
    "    scores = []\n",
    "\n",
    "    for encoder_hidden_state in encoder_hidden_states:\n",
    "        scores.append(self.v @ torch.tanh(self.W @ previous_decoder_hidden_state + self.U(encoder_hidden_state)))\n",
    "        # 1 x dec_hidden_dim @ (dec_hiddem_dim x enc_output_dim) --> 1 x enc_output_dim\n",
    "        # 1 X enc_output_dim @ enc_output_dim x 1 --> 1 x 1\n",
    "\n",
    "    scores = torch.stack(scores)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    context_vector  = attention_weights @ encoder_hidden_states\n",
    "    # 1 x enc_output_dim @ enc_output_dim x enc_output_dim --> 1 x enc_output_dim\n",
    "    return context_vector\n",
    "\n",
    "DecoderLSTMWithAttention.compute_context_vector = compute_context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81b83f",
   "metadata": {},
   "source": [
    "**Sequence-to-sequence model.** We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba3db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically, encoder/decoder hidden dimensions are the same,\n",
    "# but here we choose them differently to test our implementation.\n",
    "embedding_dim = 10\n",
    "enc_hidden_dim = 15\n",
    "dec_hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_directions = 2 if bidirectional else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea07b9",
   "metadata": {},
   "source": [
    "Now we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbef75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, enc_hidden_dim, num_enc_layers, bidirectional, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        encoder_output_dim = enc_hidden_dim * num_directions\n",
    "        self.encoder = nn.LSTM(embedding_dim, enc_hidden_dim, num_layers= num_enc_layers, bidirectional=bidirectional)\n",
    "        self.decoder = DecoderLSTMWithAttention(embedding_dim, encoder_output_dim, dec_hidden_dim)\n",
    "\n",
    "        self.encoder.reset_parameters()\n",
    "        self.decoder.reset_parameters()\n",
    "\n",
    "    def forward(self, x, y, h0, c0):\n",
    "        encoder_hidden_states, _ = self.encoder(x, (h0, c0))\n",
    "        decoder_outputs, (hn_dec, cn_dec) = self.decoder(y, encoder_hidden_states)\n",
    "        return decoder_outputs, (hn_dec, cn_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3038",
   "metadata": {},
   "source": [
    "Try your Module with an example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8f6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2seqLSTMWithAttention(embedding_dim, enc_hidden_dim, num_layers, bidirectional, dec_hidden_dim)\n",
    "x = torch.randn(10, embedding_dim)\n",
    "y = torch.randn(8, embedding_dim)\n",
    "h0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "c0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d471622",
   "metadata": {},
   "source": [
    "Create a subclass of your decoder LSTM that implements the other type of attention (additive or multiplicative) that you haven't implemented above. What do you need to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e999d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTMWithAdditiveAttention(DecoderLSTMWithAttention):\n",
    "    # or:  DecoderLSTMWithMultiplicativeAttention\n",
    "    \n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__(input_dim, enc_output_dim, dec_hidden_dim)\n",
    "        self.W = nn.Parameter(torch.randn(dec_hidden_dim, dec_hidden_dim))\n",
    "        self.U = nn.Parameter(torch.randn(dec_hidden_dim, enc_output_dim))\n",
    "        self.v = nn.Parameter(torch.randn(dec_hidden_dim))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Instead of calling super().reset_parameters()\n",
    "        # Directly initialize what you need\n",
    "        self.down_project.reset_parameters()\n",
    "        nn.init.normal_(self.U, mean=0, std=1)\n",
    "        nn.init.normal_(self.W, mean=0, std=1)\n",
    "        nn.init.normal_(self.v, mean=0, std=1)\n",
    "        self.cell.reset_parameters()\n",
    "\n",
    "    def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):\n",
    "        scores = []\n",
    "\n",
    "        for encoder_hidden_state in encoder_hidden_states:\n",
    "            attention_score = self.v @ torch.tanh(self.W @ previous_decoder_hidden_state + self.U @ encoder_hidden_state)\n",
    "            scores.append(attention_score)\n",
    "\n",
    "        scores = torch.stack(scores)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        context_vector  = attention_weights @ encoder_hidden_states\n",
    "        return context_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb688ab",
   "metadata": {},
   "source": [
    "We can test our implementation with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b7cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output_dim = enc_hidden_dim * num_directions\n",
    "# Uncomment the version you just implemented\n",
    "model.decoder = DecoderLSTMWithAdditiveAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "# model.decoder = DecoderLSTMWithMultiplicativeAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "model.decoder.reset_parameters()\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d368d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
