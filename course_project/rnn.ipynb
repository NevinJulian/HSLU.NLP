{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import gensim\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, trange\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from collections import Counter\n",
    "from matplotlib_venn import venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the FastText model from Hugging Face\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "\n",
    "# Load the model\n",
    "ft_model = gensim.models.fasttext.load_facebook_model(model_path)\n",
    "wv = ft_model.wv\n",
    "print(f\"Word embeddings loaded: {len(wv.key_to_index)} words, {wv.vector_size} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset splits\n",
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")  # Use last 1000 examples as validation\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")  # Use original validation as test\n",
    "\n",
    "print(f\"Dataset loaded - Train: {len(train)}, Validation: {len(valid)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames for analysis\n",
    "train_df = pd.DataFrame(train)\n",
    "valid_df = pd.DataFrame(valid)\n",
    "test_df = pd.DataFrame(test)\n",
    "\n",
    "# Sample data points\n",
    "print(\"\\nSample question:\")\n",
    "print(train_df['question'].iloc[0])\n",
    "print(\"\\nSample choices:\")\n",
    "print(train_df['choices'].iloc[0])\n",
    "print(\"\\nSample answer key:\")\n",
    "print(train_df['answerKey'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add question length to dataframes\n",
    "train_df['question_length'] = train_df['question'].apply(lambda x: len(word_tokenize(x)))\n",
    "valid_df['question_length'] = valid_df['question'].apply(lambda x: len(word_tokenize(x)))\n",
    "test_df['question_length'] = test_df['question'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Plot question length distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=train_df, x='question_length', kde=True, label='Train', alpha=0.6)\n",
    "sns.histplot(data=valid_df, x='question_length', kde=True, label='Validation', alpha=0.6)\n",
    "sns.histplot(data=test_df, x='question_length', kde=True, label='Test', alpha=0.6)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Number of tokens in question')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot answer key distributions for each split\n",
    "datasets = {\"Training\": train_df, \"Validation\": valid_df, \"Test\": test_df}\n",
    "\n",
    "for i, (split, df) in enumerate(datasets.items(), 1):\n",
    "    answer_counts = Counter(df['answerKey'])\n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.bar(answer_counts.keys(), answer_counts.values())\n",
    "    plt.title(f\"{split} Answer Key Distribution\")\n",
    "    plt.xlabel(\"Answer Labels\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # Add count labels\n",
    "    for label, count in answer_counts.items():\n",
    "        plt.text(label, count + 5, str(count), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    \"\"\"Extract the question type based on first word or common question words\"\"\"\n",
    "    question = question.lower().strip()\n",
    "    question_words = ['what', 'which', 'who', 'how', 'why', 'when', 'where']\n",
    "    \n",
    "    for word in question_words:\n",
    "        if question.startswith(word) or f\" {word} \" in question:\n",
    "            return word\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "# Add question type to dataframes\n",
    "train_df['question_type'] = train_df['question'].apply(get_question_type)\n",
    "valid_df['question_type'] = valid_df['question'].apply(get_question_type)\n",
    "test_df['question_type'] = test_df['question'].apply(get_question_type)\n",
    "\n",
    "# Plot question type distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (split, df) in enumerate(datasets.items(), 1):\n",
    "    question_type_counts = Counter(df['question_type'])\n",
    "    sorted_types = sorted(question_type_counts.keys())\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.bar(sorted_types, [question_type_counts[t] for t in sorted_types])\n",
    "    plt.title(f\"{split} Question Type Distribution\")\n",
    "    plt.xlabel(\"Question Type\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(texts):\n",
    "    \"\"\"Extract unique vocabulary from a list of texts\"\"\"\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "# Get vocabulary from train and test sets\n",
    "train_vocab = get_vocabulary(train_df['question'].tolist())\n",
    "test_vocab = get_vocabulary(test_df['question'].tolist())\n",
    "\n",
    "# Calculate vocabulary overlap\n",
    "overlap = len(train_vocab.intersection(test_vocab))\n",
    "train_only = len(train_vocab - test_vocab)\n",
    "test_only = len(test_vocab - train_vocab)\n",
    "\n",
    "# Plot Venn diagram\n",
    "plt.figure(figsize=(8, 6))\n",
    "venn2(subsets=(train_only, test_only, overlap), \n",
    "      set_labels=('Train Vocabulary', 'Test Vocabulary'))\n",
    "plt.title('Vocabulary Overlap Between Train and Test Sets')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Train vocabulary size: {len(train_vocab)}\")\n",
    "print(f\"Test vocabulary size: {len(test_vocab)}\")\n",
    "print(f\"Vocabulary overlap: {overlap} words ({overlap/len(train_vocab)*100:.2f}% of train vocabulary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: tokenize only, preserving case, punctuation, and all words\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(f\"Input must be a string, got {type(text).__name__} instead\")\n",
    "    \n",
    "    if not text or text.isspace():\n",
    "        return []\n",
    "\n",
    "    # Simple tokenization using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEmbeddingDataset(Dataset):\n",
    "    \"\"\"Dataset for CommonsenseQA with averaged word embeddings\"\"\"\n",
    "    def __init__(self, hf_dataset, word_vectors, cache_path=None):\n",
    "        self.data = hf_dataset\n",
    "        self.wv = word_vectors\n",
    "        self.embedding_dim = word_vectors.vector_size\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Load cache if provided and exists\n",
    "        if cache_path and os.path.exists(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    self.cache = pickle.load(f)\n",
    "                print(f\"Loaded {len(self.cache)} cached embeddings\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load cache: {e}\")\n",
    "                self.cache = {}\n",
    "        \n",
    "        self.cache_path = cache_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"Get averaged word embedding for text with caching\"\"\"\n",
    "        # Return from cache if available\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "        \n",
    "        # Process text and compute embedding\n",
    "        tokens = preprocess_text(text)\n",
    "        \n",
    "        # Filter tokens to only those in vocabulary\n",
    "        valid_tokens = [word for word in tokens if word in self.wv]\n",
    "        \n",
    "        if not valid_tokens:\n",
    "            # Return zeros if no valid tokens\n",
    "            embedding = np.zeros(self.embedding_dim)\n",
    "        else:\n",
    "            # Simple averaging of word vectors\n",
    "            word_vectors = [self.wv[word] for word in valid_tokens]\n",
    "            embedding = np.mean(word_vectors, axis=0)\n",
    "        \n",
    "        # Store in cache\n",
    "        self.cache[text] = embedding\n",
    "        return embedding\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # Get question embedding\n",
    "        question_embedding = self.get_embedding(example[\"question\"])\n",
    "        \n",
    "        # Get choice embeddings\n",
    "        choice_embeddings = [self.get_embedding(choice) for choice in example[\"choices\"][\"text\"]]\n",
    "        \n",
    "        # Convert answer key to index (A->0, B->1, etc.)\n",
    "        answer_index = ord(example[\"answerKey\"]) - ord(\"A\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        question_tensor = torch.tensor(question_embedding).float()\n",
    "        choices_tensor = torch.tensor(choice_embeddings).float()\n",
    "        answer_tensor = torch.tensor(answer_index).long()\n",
    "        \n",
    "        return question_tensor, choices_tensor, answer_tensor\n",
    "    \n",
    "    def save_cache(self):\n",
    "        \"\"\"Save embedding cache to disk if cache_path is set\"\"\"\n",
    "        if self.cache_path:\n",
    "            with open(self.cache_path, 'wb') as f:\n",
    "                pickle.dump(self.cache, f)\n",
    "            print(f\"Saved {len(self.cache)} embeddings to cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_embedding_dataset = QAEmbeddingDataset(\n",
    "    train, \n",
    "    wv,\n",
    "    cache_path='train_embeddings.pkl'\n",
    ")\n",
    "\n",
    "valid_embedding_dataset = QAEmbeddingDataset(\n",
    "    valid, \n",
    "    wv,\n",
    "    cache_path='valid_embeddings.pkl'\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_embedding_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_embedding_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Created train loader with {len(train_loader)} batches\")\n",
    "print(f\"Created validation loader with {len(valid_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Question-answering model using word embeddings\n",
    "    \n",
    "    Architecture:\n",
    "    1. Separate projection layers for question and choices\n",
    "    2. Question-choice interaction through concatenation\n",
    "    3. Two fully-connected layers for scoring\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim=128, dropout_rate=0.2):\n",
    "        super(QAEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Projection layers\n",
    "        self.question_projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.choice_projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, question, choices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            question: [batch_size, embedding_dim] - Question embeddings\n",
    "            choices: [batch_size, num_choices, embedding_dim] - Choice embeddings\n",
    "            \n",
    "        Returns:\n",
    "            [batch_size, num_choices] - Logits for each choice\n",
    "        \"\"\"\n",
    "        batch_size, num_choices, _ = choices.size()\n",
    "        \n",
    "        # Project question\n",
    "        question_hidden = self.question_projection(question)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Project all choices (flatten batch and choices dimensions first)\n",
    "        choices_flat = choices.view(batch_size * num_choices, -1)\n",
    "        choices_hidden = self.choice_projection(choices_flat)\n",
    "        choices_hidden = choices_hidden.view(batch_size, num_choices, -1)\n",
    "        \n",
    "        # Expand question to match choices dimension\n",
    "        question_expanded = question_hidden.unsqueeze(1).expand(-1, num_choices, -1)\n",
    "        \n",
    "        # Concatenate question and choices\n",
    "        combined = torch.cat((question_expanded, choices_hidden), dim=2)\n",
    "        \n",
    "        # Flatten for classifier\n",
    "        combined_flat = combined.view(batch_size * num_choices, -1)\n",
    "        \n",
    "        # Get scores for each choice\n",
    "        scores_flat = self.classifier(combined_flat)\n",
    "        scores = scores_flat.view(batch_size, num_choices)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(model, train_loader, valid_loader, epochs=30, lr=1e-3,\n",
    "                          weight_decay=1e-5, log_wandb=True):\n",
    "    \"\"\"\n",
    "    Train the embedding-based QA model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        valid_loader: Validation data loader\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay coefficient\n",
    "        log_wandb: Whether to log metrics to W&B\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and best validation accuracy\n",
    "    \"\"\"\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Setup WandB if requested\n",
    "    if log_wandb:\n",
    "        wandb.init(\n",
    "            project=\"commonsense-qa-embeddings\",\n",
    "            name=f\"embedding-model-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "            config={\n",
    "                \"model\": \"qa_embedding_model\",\n",
    "                \"embedding_dim\": model.embedding_dim,\n",
    "                \"hidden_dim\": model.hidden_dim,\n",
    "                \"epochs\": epochs,\n",
    "                \"learning_rate\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "            }\n",
    "        )\n",
    "        wandb.watch(model)\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for question_batch, choices_batch, answer_batch in tqdm(train_loader, desc=\"Training\"):\n",
    "            # Move data to device\n",
    "            question_batch = question_batch.to(device)\n",
    "            choices_batch = choices_batch.to(device)\n",
    "            answer_batch = answer_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(question_batch, choices_batch)\n",
    "            loss = criterion(outputs, answer_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += answer_batch.size(0)\n",
    "            train_correct += (predicted == answer_batch).sum().item()\n",
    "        \n",
    "        train_accuracy = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for question_batch, choices_batch, answer_batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "                # Move data to device\n",
    "                question_batch = question_batch.to(device)\n",
    "                choices_batch = choices_batch.to(device)\n",
    "                answer_batch = answer_batch.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(question_batch, choices_batch)\n",
    "                loss = criterion(outputs, answer_batch)\n",
    "                \n",
    "                # Statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += answer_batch.size(0)\n",
    "                val_correct += (predicted == answer_batch).sum().item()\n",
    "        \n",
    "        val_accuracy = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            print(f\"Validation accuracy improved from {best_val_accuracy:.4f} to {val_accuracy:.4f}\")\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "            }, \"checkpoints/best_embedding_model.pt\")\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Log to WandB\n",
    "        if log_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "    \n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    print(f\"Training completed in {training_time:.2f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    if log_wandb:\n",
    "        wandb.run.summary[\"best_val_accuracy\"] = best_val_accuracy\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "embedding_dim = wv.vector_size\n",
    "hidden_dim = 128\n",
    "dropout_rate = 0.2\n",
    "\n",
    "embedding_model = QAEmbeddingModel(\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "embedding_model = embedding_model.to(device)\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting embedding model training...\")\n",
    "trained_embedding_model, best_embedding_accuracy = train_embedding_model(\n",
    "    embedding_model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "print(f\"Embedding model training complete! Best validation accuracy: {best_embedding_accuracy:.4f}\")\n",
    "\n",
    "# Save cache for future use\n",
    "train_embedding_dataset.save_cache()\n",
    "valid_embedding_dataset.save_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QARNNDataset(Dataset):\n",
    "    \"\"\"Dataset for CommonsenseQA with sequence processing for RNN\"\"\"\n",
    "    def __init__(self, hf_dataset, word_vectors, max_seq_length=50, cache_path=None):\n",
    "        self.data = hf_dataset\n",
    "        self.wv = word_vectors\n",
    "        self.embedding_dim = word_vectors.vector_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "        self.SEP_TOKEN = \"<SEP>\"\n",
    "        \n",
    "        # Build vocabulary mapping\n",
    "        self.word_to_idx = {\n",
    "            self.PAD_TOKEN: 0,\n",
    "            self.UNK_TOKEN: 1,\n",
    "            self.SEP_TOKEN: 2\n",
    "        }\n",
    "        \n",
    "        # Load cache if provided and exists\n",
    "        if cache_path and os.path.exists(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    cache_data = pickle.load(f)\n",
    "                    self.cache = cache_data.get('sequences', {})\n",
    "                    self.word_to_idx = cache_data.get('vocab', self.word_to_idx)\n",
    "                print(f\"Loaded {len(self.cache)} cached sequences\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load cache: {e}\")\n",
    "                self.cache = {}\n",
    "        \n",
    "        # Add common words to vocab if not already loaded from cache\n",
    "        if len(self.word_to_idx) <= 3:\n",
    "            self._build_vocab()\n",
    "        \n",
    "        self.cache_path = cache_path\n",
    "        print(f\"Vocabulary size: {len(self.word_to_idx)}\")\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Build vocabulary from dataset\"\"\"\n",
    "        print(\"Building vocabulary from dataset...\")\n",
    "        idx = 3  # Start after special tokens\n",
    "        \n",
    "        # Process a sample of the dataset to build vocabulary\n",
    "        for example_idx in trange(min(1000, len(self.data))):\n",
    "            example = self.data[example_idx]\n",
    "            \n",
    "            # Process question\n",
    "            for token in preprocess_text(example[\"question\"]):\n",
    "                if token not in self.word_to_idx and token in self.wv:\n",
    "                    self.word_to_idx[token] = idx\n",
    "                    idx += 1\n",
    "            \n",
    "            # Process choices\n",
    "            for choice in example[\"choices\"][\"text\"]:\n",
    "                for token in preprocess_text(choice):\n",
    "                    if token not in self.word_to_idx and token in self.wv:\n",
    "                        self.word_to_idx[token] = idx\n",
    "                        idx += 1\n",
    "        \n",
    "        print(f\"Built vocabulary with {len(self.word_to_idx)} words\")\n",
    "    \n",
    "    def _get_word_idx(self, word):\n",
    "        \"\"\"Get vocabulary index for word, using UNK for unknown words\"\"\"\n",
    "        return self.word_to_idx.get(word, self.word_to_idx[self.UNK_TOKEN])\n",
    "    \n",
    "    def prepare_sequence(self, question, choice):\n",
    "        \"\"\"Tokenize and prepare a question-choice sequence\"\"\"\n",
    "        # Create cache key\n",
    "        cache_key = f\"{question}:{choice}\"\n",
    "        \n",
    "        # Check if already in cache\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Tokenize\n",
    "        question_tokens = preprocess_text(question)\n",
    "        choice_tokens = preprocess_text(choice)\n",
    "        \n",
    "        # Combine with separator token\n",
    "        combined_tokens = question_tokens + [self.SEP_TOKEN] + choice_tokens\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(combined_tokens) > self.max_seq_length:\n",
    "            combined_tokens = combined_tokens[:self.max_seq_length]\n",
    "        \n",
    "        # Convert to indices\n",
    "        token_ids = [self._get_word_idx(token) for token in combined_tokens]\n",
    "        \n",
    "        # Save in cache\n",
    "        self.cache[cache_key] = token_ids\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # Get question and choices\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"][\"text\"]\n",
    "        \n",
    "        # Prepare sequences for each question-choice pair\n",
    "        sequences = []\n",
    "        sequence_lengths = []\n",
    "        \n",
    "        for choice in choices:\n",
    "            token_ids = self.prepare_sequence(question, choice)\n",
    "            sequences.append(torch.tensor(token_ids, dtype=torch.long))\n",
    "            sequence_lengths.append(len(token_ids))\n",
    "        \n",
    "        # Convert answer key to index\n",
    "        answer_index = ord(example[\"answerKey\"]) - ord(\"A\")\n",
    "        answer = torch.tensor(answer_index, dtype=torch.long)\n",
    "        \n",
    "        return sequences, sequence_lengths, answer\n",
    "    \n",
    "    def get_embedding_matrix(self):\n",
    "        \"\"\"Create embedding matrix for initializing the embedding layer\"\"\"\n",
    "        vocab_size = len(self.word_to_idx)\n",
    "        embedding_matrix = torch.zeros(vocab_size, self.embedding_dim)\n",
    "        \n",
    "        # Fill embedding matrix with pre-trained vectors where available\n",
    "        for word, idx in self.word_to_idx.items():\n",
    "            if word == self.PAD_TOKEN:\n",
    "                # Zero vector for padding\n",
    "                pass\n",
    "            elif word == self.UNK_TOKEN or word == self.SEP_TOKEN:\n",
    "                # Random initialization for special tokens\n",
    "                embedding_matrix[idx] = torch.randn(self.embedding_dim) * 0.1\n",
    "            else:\n",
    "                # Use pre-trained vectors for known words\n",
    "                try:\n",
    "                    embedding_matrix[idx] = torch.tensor(self.wv[word])\n",
    "                except (KeyError, ValueError):\n",
    "                    embedding_matrix[idx] = torch.randn(self.embedding_dim) * 0.1\n",
    "        \n",
    "        return embedding_matrix\n",
    "    \n",
    "    def save_cache(self):\n",
    "        \"\"\"Save sequences and vocabulary to cache\"\"\"\n",
    "        if self.cache_path:\n",
    "            cache_data = {\n",
    "                'sequences': self.cache,\n",
    "                'vocab': self.word_to_idx\n",
    "            }\n",
    "            with open(self.cache_path, 'wb') as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(f\"Saved {len(self.cache)} sequences and {len(self.word_to_idx)} vocabulary items to cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_rnn_batch(batch):\n",
    "    \"\"\"Collate function for RNN batches\"\"\"\n",
    "    # Unpack batch\n",
    "    all_sequences = []\n",
    "    all_lengths = []\n",
    "    all_answers = []\n",
    "    \n",
    "    for sequences, lengths, answer in batch:\n",
    "        all_sequences.append(sequences)\n",
    "        all_lengths.append(lengths)\n",
    "        all_answers.append(answer)\n",
    "    \n",
    "    # Stack answers\n",
    "    answers_tensor = torch.stack(all_answers)\n",
    "    \n",
    "    return all_sequences, all_lengths, answers_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN datasets\n",
    "train_rnn_dataset = QARNNDataset(\n",
    "    train, \n",
    "    wv,\n",
    "    max_seq_length=50,\n",
    "    cache_path='train_rnn_cache.pkl'\n",
    ")\n",
    "\n",
    "valid_rnn_dataset = QARNNDataset(\n",
    "    valid, \n",
    "    wv,\n",
    "    max_seq_length=50,\n",
    "    cache_path='valid_rnn_cache.pkl'\n",
    ")\n",
    "\n",
    "# Create RNN data loaders\n",
    "train_rnn_loader = DataLoader(\n",
    "    train_rnn_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_rnn_batch,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_rnn_loader = DataLoader(\n",
    "    valid_rnn_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_rnn_batch,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Created RNN train loader with {len(train_rnn_loader)} batches\")\n",
    "print(f\"Created RNN validation loader with {len(valid_rnn_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QARNNModel(nn.Module):\n",
    "    \"\"\"RNN model for CommonsenseQA with 2-layer LSTM and 2-layer classifier\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.3):\n",
    "        super(QARNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # 2-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 2-layer classifier\n",
    "        lstm_output_dim = hidden_dim * 2  # bidirectional = *2\n",
    "        self.fc1 = nn.Linear(lstm_output_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, batch_sequences, batch_lengths):\n",
    "        \"\"\"\n",
    "        Process batch efficiently using choice-based batching\n",
    "        \n",
    "        Args:\n",
    "            batch_sequences: List of lists of tensor sequences\n",
    "            batch_lengths: List of lists of sequence lengths\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of logits [batch_size, num_choices]\n",
    "        \"\"\"\n",
    "        batch_size = len(batch_sequences)\n",
    "        num_choices = len(batch_sequences[0])\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        # Create tensor to store results\n",
    "        all_logits = torch.zeros((batch_size, num_choices), device=device)\n",
    "        \n",
    "        # Process each choice across all examples together\n",
    "        for choice_idx in range(num_choices):\n",
    "            # Collect all sequences for this choice\n",
    "            choice_sequences = []\n",
    "            \n",
    "            for batch_idx in range(batch_size):\n",
    "                seq = batch_sequences[batch_idx][choice_idx].to(device)\n",
    "                if seq.dtype != torch.long:\n",
    "                    seq = seq.long()\n",
    "                choice_sequences.append(seq)\n",
    "            \n",
    "            # Get lengths before padding\n",
    "            lengths = torch.tensor([seq.size(0) for seq in choice_sequences], device=device)\n",
    "            \n",
    "            # Pad sequences to same length\n",
    "            padded_sequences = pad_sequence(choice_sequences, batch_first=True, padding_value=0)\n",
    "            \n",
    "            # Sort by length for packed_padded_sequence\n",
    "            sorted_lengths, sorted_indices = lengths.sort(descending=True)\n",
    "            sorted_padded = padded_sequences[sorted_indices]\n",
    "            \n",
    "            # Embed sequences\n",
    "            embedded = self.embedding(sorted_padded)\n",
    "            \n",
    "            # Pack padded sequences for efficient LSTM\n",
    "            packed = pack_padded_sequence(\n",
    "                embedded, \n",
    "                sorted_lengths.cpu(), \n",
    "                batch_first=True,\n",
    "                enforce_sorted=True\n",
    "            )\n",
    "            \n",
    "            # Process through LSTM\n",
    "            _, (hidden, _) = self.lstm(packed)\n",
    "            \n",
    "            # Get final hidden states from both directions\n",
    "            final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "            \n",
    "            # Process through classifier\n",
    "            x = self.fc1(final_hidden)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            logits = self.fc2(x).squeeze(-1)\n",
    "            \n",
    "            # Restore original order\n",
    "            _, unsorted_indices = sorted_indices.sort(0)\n",
    "            unsorted_logits = logits[unsorted_indices]\n",
    "            \n",
    "            # Store results\n",
    "            all_logits[:, choice_idx] = unsorted_logits\n",
    "        \n",
    "        return all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_model(model, train_loader, valid_loader, epochs=30, lr=1e-3,\n",
    "                    weight_decay=1e-6, log_wandb=True):\n",
    "    \"\"\"\n",
    "    Train the RNN-based QA model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: Training data loader\n",
    "        valid_loader: Validation data loader\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay coefficient\n",
    "        log_wandb: Whether to log metrics to W&B\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and best validation accuracy\n",
    "    \"\"\"\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # One-cycle learning rate scheduler\n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.2,  # 20% warmup\n",
    "        div_factor=25,\n",
    "        final_div_factor=1000,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # Setup WandB if requested\n",
    "    if log_wandb:\n",
    "        wandb.init(\n",
    "            project=\"commonsense-qa-rnn\",\n",
    "            name=f\"rnn-model-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "            config={\n",
    "                \"model\": \"qa_rnn_model\",\n",
    "                \"embedding_dim\": model.embedding_dim,\n",
    "                \"hidden_dim\": model.hidden_dim,\n",
    "                \"epochs\": epochs,\n",
    "                \"learning_rate\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "            }\n",
    "        )\n",
    "        wandb.watch(model)\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        with tqdm(train_loader, desc=\"Training\") as progress_bar:\n",
    "            for batch_sequences, batch_lengths, answers in progress_bar:\n",
    "                # Move answers to device\n",
    "                answers = answers.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_sequences, batch_lengths)\n",
    "                loss = criterion(outputs, answers)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Step scheduler every batch for OneCycleLR\n",
    "                \n",
    "                # Statistics\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_total += answers.size(0)\n",
    "                train_correct += (predicted == answers).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{train_correct/train_total:.4f}\"\n",
    "                })\n",
    "        \n",
    "        train_accuracy = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_sequences, batch_lengths, answers in tqdm(valid_loader, desc=\"Validation\"):\n",
    "                # Move answers to device\n",
    "                answers = answers.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(batch_sequences, batch_lengths)\n",
    "                loss = criterion(outputs, answers)\n",
    "                \n",
    "                # Statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += answers.size(0)\n",
    "                val_correct += (predicted == answers).sum().item()\n",
    "        \n",
    "        val_accuracy = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            print(f\"Validation accuracy improved from {best_val_accuracy:.4f} to {val_accuracy:.4f}\")\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "            }, \"checkpoints/best_rnn_model.pt\")\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Log to WandB\n",
    "        if log_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"train_accuracy\": train_accuracy,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "    \n",
    "    training_time = (time.time() - start_time) / 60\n",
    "    print(f\"Training completed in {training_time:.2f} minutes\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "    \n",
    "    if log_wandb:\n",
    "        wandb.run.summary[\"best_val_accuracy\"] = best_val_accuracy\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RNN model\n",
    "embedding_dim = 300\n",
    "hidden_dim = 128\n",
    "dropout_rate = 0.2\n",
    "\n",
    "rnn_model = QARNNModel(\n",
    "    vocab_size=len(train_rnn_dataset.word_to_idx),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Initialize embedding layer with pre-trained word vectors\n",
    "embedding_matrix = train_rnn_dataset.get_embedding_matrix()\n",
    "rnn_model.embedding.weight.data.copy_(embedding_matrix)\n",
    "\n",
    "# Move model to device\n",
    "rnn_model = rnn_model.to(device)\n",
    "print(rnn_model)\n",
    "\n",
    "# Train the RNN model\n",
    "print(\"Starting RNN model training...\")\n",
    "trained_rnn_model, best_rnn_accuracy = train_rnn_model(\n",
    "    rnn_model,\n",
    "    train_rnn_loader,\n",
    "    valid_rnn_loader,\n",
    "    epochs=30,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "print(f\"RNN model training complete! Best validation accuracy: {best_rnn_accuracy:.4f}\")\n",
    "\n",
    "# Save cache for future use\n",
    "train_rnn_dataset.save_cache()\n",
    "valid_rnn_dataset.save_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
