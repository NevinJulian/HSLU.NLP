{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP.F2501 Course Project 1 (Word embeddings and RNNs)\n",
    "\n",
    "Nevin Helfenstein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I present my solution to the CommonsenseQA task. I'll train a model using word embeddings, RNNs, and other NLP techniques to achieve the best possible performance.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The CommonsenseQA dataset [(Talmor et al., 2019)](https://aclanthology.org/N19-1421/) contains 12,247 multiple-choice questions specifically designed to test commonsense reasoning. Unlike standard QA tasks, these questions require prior knowledge about how concepts relate in the real world.\n",
    "\n",
    "Questions were created by extracting related concepts from ConceptNet and having crowd-workers author questions that require distinguishing between them. This methodology produced challenging questions that often cannot be answered through simple pattern matching.\n",
    "\n",
    "The best baseline in the original paper (BERT-large) achieved only 56% accuracy compared to human performance of 89%, shwoing the difficulty of encoding human-like commonsense reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "First we import all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/maiko/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaikotrede\u001b[0m (\u001b[33mmaikotrede-hochschule-luzern\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import wandb\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim.downloader as api\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WandDB\n",
    "\n",
    "We set up the configuration to W&B for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Correct configuration for the new wanddb project\n",
    "\n",
    "wandb.login(key=\"\")\n",
    "wandb_logger = WandbLogger(project=\"experiment-tracking\")\n",
    "\n",
    "def init_wandb_run(project_name, run_name, config_dict):\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=config_dict,\n",
    "    )\n",
    "    return run\n",
    "\n",
    "\n",
    "#### Example Run\n",
    "experiment_1_config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"optimizer\": \"adam\"\n",
    "}\n",
    "run1 = init_wandb_run(\"experiment-tracking\", \"experiment_1\", experiment_1_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed variables\n",
    "\n",
    "We set the random seed for all the necessary configurations  to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and embedding model\n",
    "\n",
    "I've selected GloVe embeddings for this project for the following reasons:\n",
    "\n",
    "* Merges global statistics with local context data\n",
    "* Strong performance on semantic similarity benchmarks relevant to commonsense reasoning\n",
    "* 300-dimensional vectors balance representation ability with efficiency\n",
    "* Strongly established in NLP community with good documentation\n",
    "* Effectively models conceptual relationships for answering commonsense questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tokenizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load GloVe embeddings using gensim downloader (300d Wikipedia + Gigaword model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to get embeddings for words (Return zero vector for OOV words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(word):\n",
    "    try:\n",
    "        return glove_model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splits\n",
    "\n",
    "The data is available on Hugging Face: [Data](https://huggingface.co/datasets/tau/commonsense_qa).\n",
    "Only the train and validation splits have an answer key, because of that we will use our own dataset splits.\n",
    "We use all of the datasamples as the train set except for the last 1000 which we set as the validation set. The original validation set is set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert to DataFrames for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "valid_df = pd.DataFrame(valid)\n",
    "test_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check basic statistics such as average question and choice lenght for each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, name):\n",
    "    \"\"\"Analyze basic statistics of a dataset split\"\"\"\n",
    "    print(f\"=== {name} Set Statistics ===\")\n",
    "    print(f\"Number of examples: {len(df)}\")\n",
    "    \n",
    "    # Question statistics\n",
    "    df['question_tokens'] = df['question'].apply(lambda x: word_tokenize(x))\n",
    "    df['question_length'] = df['question_tokens'].apply(len)\n",
    "    \n",
    "    # Answer choices statistics\n",
    "    df['choices_length'] = df['choices'].apply(lambda x: [len(word_tokenize(choice['text'])) for choice in x])\n",
    "    df['avg_choice_length'] = df['choices_length'].apply(np.mean)\n",
    "    \n",
    "    print(f\"Average question length: {df['question_length'].mean():.2f} tokens\")\n",
    "    print(f\"Average answer choice length: {df['avg_choice_length'].mean():.2f} tokens\")\n",
    "    print(f\"Min/Max question length: {df['question_length'].min()}/{df['question_length'].max()} tokens\")\n",
    "    \n",
    "    # Find correct answer position\n",
    "    df['correct_answer_idx'] = df.apply(lambda row: next((i for i, choice in enumerate(row['choices']) \n",
    "                                                         if choice['label'] == row['answerKey']), -1), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = analyze_dataset(train_df, \"Training\")\n",
    "valid_df = analyze_dataset(valid_df, \"Validation\")\n",
    "test_df = analyze_dataset(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the question lenght distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(data=train_df, x='question_length', kde=True, label='Train', alpha=0.6)\n",
    "sns.histplot(data=valid_df, x='question_length', kde=True, label='Validation', alpha=0.6)\n",
    "sns.histplot(data=test_df, x='question_length', kde=True, label='Test', alpha=0.6)\n",
    "\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Number of tokens in question')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.savefig('question_length_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the answer length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.histplot(data=train_df, x='avg_choice_length', kde=True, label='Train', alpha=0.6)\n",
    "sns.histplot(data=valid_df, x='avg_choice_length', kde=True, label='Validation', alpha=0.6)\n",
    "sns.histplot(data=test_df, x='avg_choice_length', kde=True, label='Test', alpha=0.6)\n",
    "\n",
    "plt.title('Distribution of Answer Choice Lengths')\n",
    "plt.xlabel('Average number of tokens in answer choices')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.savefig('answer_length_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count correct answer keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "train_pos_counts = Counter(train_df['correct_answer_idx'])\n",
    "valid_pos_counts = Counter(valid_df['correct_answer_idx'])\n",
    "test_pos_counts = Counter(test_df['correct_answer_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert that to percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_percent = {k: v/len(train_df)*100 for k, v in train_pos_counts.items()}\n",
    "valid_pos_percent = {k: v/len(valid_df)*100 for k, v in valid_pos_counts.items()}\n",
    "test_pos_percent = {k: v/len(test_df)*100 for k, v in test_pos_counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame again for better plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame({\n",
    "    'Train': [train_pos_percent.get(i, 0) for i in range(5)],\n",
    "    'Validation': [valid_pos_percent.get(i, 0) for i in range(5)],\n",
    "    'Test': [test_pos_percent.get(i, 0) for i in range(5)]\n",
    "}, index=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "pos_df.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Distribution of Correct Answer Positions')\n",
    "plt.xlabel('Answer Position')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.legend()\n",
    "plt.savefig('answer_position_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word clouds for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(texts, title, filename):\n",
    "    \"\"\"Create and save wordcloud from a list of texts\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    all_text = ' '.join(texts)\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white',\n",
    "                         max_words=100, \n",
    "                         contour_width=3).generate(all_text)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "create_wordcloud(train_df['question'].tolist(), 'Word Cloud - Training Questions', 'train_question_wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze question types (what, how, why, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    \"\"\"Extract the question word from a question\"\"\"\n",
    "    question = question.lower().strip()\n",
    "    question_words = ['what', 'which', 'who', 'how', 'why', 'when', 'where']\n",
    "    \n",
    "    for word in question_words:\n",
    "        if question.startswith(word) or f\" {word} \" in question:\n",
    "            return word\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "train_df['question_type'] = train_df['question'].apply(get_question_type)\n",
    "valid_df['question_type'] = valid_df['question'].apply(get_question_type)\n",
    "test_df['question_type'] = test_df['question'].apply(get_question_type)\n",
    "\n",
    "plt.figure(figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_counts = Counter(train_df['question_type'])\n",
    "valid_type_counts = Counter(valid_df['question_type'])\n",
    "test_type_counts = Counter(test_df['question_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converte to percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_percent = {k: v/len(train_df)*100 for k, v in train_type_counts.items()}\n",
    "valid_type_percent = {k: v/len(valid_df)*100 for k, v in valid_type_counts.items()}\n",
    "test_type_percent = {k: v/len(test_df)*100 for k, v in test_type_counts.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the question type distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_types = set(list(train_type_counts.keys()) + list(valid_type_counts.keys()) + list(test_type_counts.keys()))\n",
    "type_df = pd.DataFrame({\n",
    "    'Train': [train_type_percent.get(t, 0) for t in common_types],\n",
    "    'Validation': [valid_type_percent.get(t, 0) for t in common_types],\n",
    "    'Test': [test_type_percent.get(t, 0) for t in common_types]\n",
    "}, index=common_types)\n",
    "\n",
    "type_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Distribution of Question Types')\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.legend()\n",
    "plt.savefig('question_type_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS (Part-of-Speach) Tag analysis for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pos_tags(texts, n=10):\n",
    "    \"\"\"Analyze the most common POS tags in a list of texts\"\"\"\n",
    "    all_pos = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        tags = pos_tag(tokens)\n",
    "        all_pos.extend([tag for _, tag in tags])\n",
    "    \n",
    "    return Counter(all_pos).most_common(n)\n",
    "\n",
    "# Get most common POS tags in train questions\n",
    "train_pos = analyze_pos_tags(train_df['question'].tolist())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "pos_df = pd.DataFrame(train_pos, columns=['POS Tag', 'Count'])\n",
    "sns.barplot(x='POS Tag', y='Count', data=pos_df)\n",
    "plt.title('Most Common POS Tags in Training Questions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pos_tag_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze vocabulary overlap between train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(texts):\n",
    "    \"\"\"Get vocabulary from a list of texts\"\"\"\n",
    "    vocab = set()\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "train_vocab = get_vocab(train_df['question'].tolist())\n",
    "test_vocab = get_vocab(test_df['question'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate overlap and plot venn diagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = len(train_vocab.intersection(test_vocab))\n",
    "train_only = len(train_vocab - test_vocab)\n",
    "test_only = len(test_vocab - train_vocab)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "venn2(subsets=(train_only, test_only, overlap), \n",
    "      set_labels=('Train Vocabulary', 'Test Vocabulary'))\n",
    "plt.title('Vocabulary Overlap Between Train and Test Sets')\n",
    "plt.savefig('vocab_overlap.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Train vocabulary size: {len(train_vocab)}\")\n",
    "print(f\"Test vocabulary size: {len(test_vocab)}\")\n",
    "print(f\"Vocabulary overlap: {overlap} words ({overlap/len(train_vocab)*100:.2f}% of train vocab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86da2832ec43400e9ef014e993a90840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db6e48045c84d449f4806841721a420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd51640fd42a44c980b167ab1ac0c1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d066ba801764a9fa6f410944cde03be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e929721f8fc415ead9556edaa7a7b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2612fb87e740fdab2742a42e3ea71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d538a5009944babbbc08041e27ed6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
