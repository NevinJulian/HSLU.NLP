{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cf81d1",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN\n",
    "In this exercise, we implement a sequence-to-sequence RNN (without attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827d5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6923b",
   "metadata": {},
   "source": [
    "We first define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b02ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "sequence_length = 5\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfc188",
   "metadata": {},
   "source": [
    "Create a bidirectional [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f1c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc0342",
   "metadata": {},
   "source": [
    "We create an example input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89463769",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a3a2c",
   "metadata": {},
   "source": [
    "What should the initial hidden and cell state be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf1dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "h0 = torch.randn(num_layers * num_directions, batch_size, hidden_dim)\n",
    "c0 = torch.randn(num_layers * num_directions, batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3373c",
   "metadata": {},
   "source": [
    "Now we run our LSTM. Look at the output. Explain each dimension of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 40])\n",
      "torch.Size([4, 3, 20])\n",
      "torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "print(output.shape) #shape: (sequence_length, batch_size, num_directions * hidden_dim)\n",
    "print(hn.shape) #shape: (num_layers * num_directions, batch_size, hidden_dim). All the hiddenstate of all directions in the last layer\n",
    "print(cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6d2d",
   "metadata": {},
   "source": [
    "All outputs are from the last (2nd) layer of the LSTM. If we want to have access to the hidden states of layer 1 as well, we have to run the `LSTMCell`s ourselves.\n",
    "\n",
    "When we take the above LSTM as the encoder, what is its output that serves as the input to the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5386b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40])\n",
      "tensor([[-0.0540, -0.1253,  0.0244,  0.0167, -0.1227,  0.1365, -0.0936,  0.0492,\n",
      "         -0.0372, -0.0079, -0.1196, -0.0604, -0.0784,  0.0433,  0.0422, -0.0247,\n",
      "         -0.0988, -0.0530, -0.0968,  0.1095,  0.0709, -0.2326,  0.0149, -0.0769,\n",
      "         -0.0119, -0.0247,  0.1440,  0.2190,  0.0541, -0.0787,  0.1827,  0.0479,\n",
      "          0.0584,  0.0388, -0.1106, -0.0130, -0.1142,  0.1593,  0.1667,  0.0448],\n",
      "        [-0.1083, -0.1941, -0.0036,  0.0275, -0.0780,  0.1821, -0.1785,  0.0010,\n",
      "         -0.0091, -0.0912, -0.0746, -0.0709, -0.1513,  0.0474,  0.0148,  0.0684,\n",
      "         -0.1067, -0.0838, -0.0310,  0.2014, -0.0151, -0.0247, -0.0462, -0.0524,\n",
      "          0.0555, -0.1418,  0.1199,  0.0928,  0.0257, -0.1636,  0.1063,  0.0492,\n",
      "          0.1344,  0.0449,  0.0735, -0.1845, -0.0890,  0.2883,  0.1243, -0.0300],\n",
      "        [-0.0495, -0.1347, -0.0264, -0.0526, -0.0854,  0.1002, -0.1687, -0.0203,\n",
      "          0.0117,  0.0149, -0.1350, -0.1023, -0.1300, -0.0378,  0.0242,  0.0178,\n",
      "         -0.1184,  0.0016,  0.0168,  0.1144, -0.0713, -0.2055, -0.1684, -0.0793,\n",
      "          0.0714,  0.0369,  0.1506,  0.0722,  0.0572, -0.1282,  0.1508, -0.0106,\n",
      "          0.1080,  0.0339,  0.0077, -0.0423, -0.0590,  0.0637,  0.0916, -0.0030]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoder = lstm\n",
    "\n",
    "encoder_output = torch.cat((hn[2], hn[3]), dim=1) #shape: 3x40\n",
    "#torch.stack((hn[2], hn[3]), dim=1) #shape: 3x2x20\n",
    "\n",
    "print(encoder_output.shape)\n",
    "print(encoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7afab4",
   "metadata": {},
   "source": [
    "Create a decoder LSTM with 2 layers. Why can't it be bidirectional as well? What is the hidden dimension of the decoder LSTM when you want to initialize it with the encoder output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "373c7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_dim = num_directions * hidden_dim\n",
    "decoder = nn.LSTM(input_size=embedding_dim, hidden_size=decoder_hidden_dim, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab709dc",
   "metadata": {},
   "source": [
    "Run your decoder LSTM on an example sequence. Condition it with the encoder representation of the sequence. How do we get the correct shape for the initial hidden state?\n",
    "\n",
    "**Hint:** Take a look at [Torch's tensor operations](https://pytorch.org/docs/stable/tensors.html) and compare `Torch.repeat`, `Torch.repeat_interleave` and `Tensor.expand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56965f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 40])\n",
      "torch.Size([2, 3, 40])\n",
      "torch.Size([2, 3, 40])\n"
     ]
    }
   ],
   "source": [
    "output_seq_length = 8\n",
    "\n",
    "decoder_input = torch.randn(output_seq_length, batch_size, embedding_dim)\n",
    "h0_decoder = encoder_output.unsqueeze(0).expand(2, -1, -1)\n",
    "c0_decoder = torch.zeros(num_layers, batch_size, decoder_hidden_dim)\n",
    "\n",
    "decoder_output, (hn_decoder, cn_decoder) = decoder(decoder_input, (h0_decoder, c0_decoder))\n",
    "\n",
    "print(decoder_output.shape)\n",
    "print(hn_decoder.shape)\n",
    "print(cn_decoder.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ac2ab",
   "metadata": {},
   "source": [
    "In most RNNs, the final encoder hidden state is used as the first hidden state of the decoder RNN. In some variants, it has also been concatenated with the hidden state of the previous time step at each decoder time step. In PyTorch's `nn.LSTM` implementation, we cannot easily do that, so we would have to resort to the lower-level `nn.LSTMCell` class again.\n",
    "\n",
    "Put it all together in a seq2seq LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af981a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTM(nn.Module):\n",
    "    \"\"\" Sequence-to-sequence LSTM. \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, num_encoder_layers, num_decoder_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        #initialize encoder and decoder\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_encoder_layers, bidirectional=bidirectional)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.decoder = nn.LSTM(input_size=embedding_dim, hidden_size=num_directions * hidden_dim, num_layers=num_decoder_layers)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        assert x.dim() == 3, \"Expected input of shape [sequence length, batch size, embedding dim]\"\n",
    "        batch_size = x.size(1) # x shape: [sequence length, batch size, embedding dim]\n",
    "        \n",
    "        #encoder forward\n",
    "        h0_encoder = torch.zeros(self.encoder.num_layers * num_directions, batch_size, self.encoder.hidden_size)\n",
    "        c0_encoder = torch.zeros(self.encoder.num_layers * num_directions, batch_size, self.encoder.hidden_size)\n",
    "\n",
    "        encoder_output, (hn_encoder, cn_encoder) = self.encoder(x, (h0_encoder, c0_encoder))\n",
    "\n",
    "        #decoder forward\n",
    "        encoder_output = torch.cat((hn_encoder[-2], hn_encoder[-1]), dim=-1) if bidirectional else hn_encoder[-1]\n",
    "        h0_decoder = encoder_output.unsqueeze(0).expand(self.decoder.num_layers, -1, -1)\n",
    "        c0_decoder = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size)\n",
    "\n",
    "        decoder_output, (hn_decoder, cn_decoder) = self.decoder(y, (h0_decoder, c0_decoder))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dd1ad",
   "metadata": {},
   "source": [
    "Test your seq2seq LSTM with an input sequence `x` and a ground truth output sequence `y` that the decoder tries to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74ef14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "decoder_hidden_dim = num_directions * hidden_dim\n",
    "seq2seq_lstm = Seq2seqLSTM(embedding_dim, hidden_dim, num_layers, num_layers, bidirectional)\n",
    "x = torch.randn(10, 2, embedding_dim)\n",
    "y = torch.randn(9, 2, embedding_dim)\n",
    "outputs = seq2seq_lstm(x, y)\n",
    "assert outputs.dim() == 3 and list(outputs.size()) == [9, 2, decoder_hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20920a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
